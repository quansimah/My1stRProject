# making a density plot for Y
plot(density(sim.data$Y), main = "Density Plot:Y",
ylab = "Frequency", sub = paste("Skewness:",
round(e1071::skewness(sim.data$Y),2)))
# making the polygon red
polygon(density(sim.data$Y), col = 'red')
# Skewness = 0.18
# This is also a low level of skewness
# ==============================================================================
# ======= CORRELATION ==========================================================
# let us test for correlation between both variables
cor(sim.data$Y, sim.data$X1)
# 0.774579;  this suggests a strong positive correlation between both variables
# ==============================================================================
# ======= LINEAR MODEL =========================================================
# build a linear model
linearMod <- lm(Y ~ X1, data = sim.data)
# inspect the model
print(linearMod)
#Call:
# lm(formula = Y ~ X1, data = sim.data)
# Coefficients:
#   (Intercept)           X1
#       105.308        4.436
# This translates to Yestimate = 105.308 + 4.436*X1
# ==============================================================================
# === LINEAR REGRESSION DIAGNOSTICS ============================================
# get model summary
summary(linearMod)
#> Call:
#> lm(formula = Y ~ X1, data = sim.data)
#> Residuals:
#>   Min       1Q   Median       3Q      Max
#> -13.4182  -6.1344  -0.5484   5.7165  15.5783
#> Coefficients:
#>              Estimate Std. Error t value Pr(>|t|)
#> (Intercept) 105.3075    13.2156   7.968 2.46e-10 ***
#>  X1            4.4357     0.5179   8.564 3.13e-11 ***
#>  ---
#>   Signif. codes:  0 â***â 0.001 â**â 0.01 â*â 0.05 â.â 0.1 â â 1
#> Residual standard error: 7.229 on 48 degrees of freedom
#> Multiple R-squared:  0.6044,	Adjusted R-squared:  0.5962
#>  F-statistic: 73.35 on 1 and 48 DF,  p-value: 3.133e-11
# The Coefficients estimates and model p-values are lower than 0.05
# This means, the model is statistically significant
# The Adjusted R-Squared shows that the model acoounts for 59.62% of the data
# ==============================================================================
# ==== T-STATISTIC AND P-VALUE =================================================
# Assessing the t-statistic, F-statistic, and p-values manually
# capture model summary as an object
modelSummary <- summary(linearMod)
# captured model coefficients as an object
modelCoeffs <- modelSummary$coefficients
# get estimates for regressor
beta.estimate <- modelCoeffs["X1", "Estimate"]
# get standard error for regressor
std.error <- modelCoeffs["X1", "Std. Error"]
# calculating t-statistic of the coefficient estimates
t_value <- beta.estimate/std.error
# 8.564298
# calculating the p-value of the t-statistics
p_value <- 2*pt(-abs(t_value), df = nrow(sim.data)- 2)
# 3.132848e-11
# f- statistic
f_statistic <- linearMod$fstatistic[1]
# calculating the parameters for model p-value
f<- summary(linearMod)$fstatistic
#   value    numdf    dendf
# 73.3472 1.00000 48.00000
model_p <-  pf(f[1],f[2],f[3], lower =FALSE)
# model p-value
# 3.132848e-11
# Find AIC and BIC
AIC(linearMod)
# 343.659
BIC(linearMod)
# 349.395
# ==============================================================================
# ===== PREDICTING LINEAR MODELS ===============================================
# In this section, we generate a linear model of 80% of our data
# This is then used to predict the other 20% of our data
# First, we create Training Test data
#  set seed to reproduce results of random sampling
set.seed(100)
# row indices for training data
trainingRowIndex <- sample(1:nrow(sim.data), 0.8*nrow(sim.data))
# make training Data
trainingData <- sim.data[trainingRowIndex, ]
# make test Data
testData <- sim.data[-trainingRowIndex, ]
# Build the model on training data
lmMod <- lm(Y~X1, data =trainingData)
# Review diagnostic measures
summary(lmMod)
#> Coefficients:
#> Estimate Std. Error t value Pr(>|t|)
#> (Intercept)  107.395     16.452   6.528 1.08e-07 ***
#>  X1             4.373      0.641   6.823 4.27e-08 ***
#> Residual standard error: 7.813 on 38 degrees of freedom
#> Multiple R-squared:  0.5506,	Adjusted R-squared:  0.5387
#> F-statistic: 46.55 on 1 and 38 DF,  p-value: 4.275e-08
# What does all this tell us?
# both the predictor's and model's p value are less than 0.05
# So we have a statistically significant model
# ==============================================================================
# ==== PREDICTION ACCURACY =====================================================
# use the model of the training data to predict the testData
YPred <- predict(lmMod, testData)
# make a dataframe with the actual and predicted data
actual_preds <- data.frame(cbind(actuals = testData$Y,
predicteds =YPred))
# inspect the actual_preds dataframe
head(actual_preds)
# actuals predicteds
# 3  209.0133   218.9149
# 11 225.9502   226.6082
# 18 204.2498   202.8588
# 19 217.0146   212.8311
# 21 212.0542   214.8210
# 23 206.8188   209.1757
# what is the correlation accuracy of the actual and predicted values
correlation_accuracy <- cor(actual_preds)
# 92.001 %
# calculate min_max accuracy of the model
min_max_accuracy <- mean(apply(actual_preds,1,min)/
apply(actual_preds,1,max))
# 98.31% (the higher the better)
# calculate mean absolute percentage error
mape <- mean(abs((actual_preds$predicteds - actual_preds$actuals))/
actual_preds$ actuals)
# 1.73% , mean absolute percentage deviation (the lower the better)
# ==============================================================================
# ===== K-FOLD CROSS VALIDATION ===============================================
# the R packages needed for this section are lattice and DAAG
# Our previous model performed satisfactorily
# but how can we know it will all the time
# we can rigorously test the model's performance as many times as we please
# This is done using a k-fold cross validation
# for our purposes, we use a 10 fold
# change margins back to normal so the plot fits one window
par(mfrow = c(1,1))
# make plot
cvResults <- suppressWarnings( CVlm (sim.data, form.lm = Y ~ X1,
m=10, dots=FALSE, seed=29,
legend.pos="topleft",  printit=FALSE,
main="10-Fold Cross Validation"))
# supress warning supresses the warnings
# Lines are very close and  parallel
# This confirms the linear model's performance
# find mean squared error of k-fold cross validation
attr(cvResults, 'ms')
# 53.15313 , the lower the better
# ==============================================================================
# make plot
cvResults <- suppressWarnings( CVlm (sim.data, form.lm = Y ~ X1,
m=10, dots=FALSE, seed=29,
legend.pos="topleft",  printit=FALSE,
main="10-Fold Cross Validation"))
# find mean squared error of k-fold cross validation
attr(cvResults, 'ms')
#
#
# MULTIPLE REGRESSION ANALYSIS
#  Performing a  multiple regression analysis on simulated data
#
# ------ RUN SCRIPT ------------------------------------------------------------
# for this section, we need the Rpackages AER and MASS
# we also need  to run the linear regression script since we will be comparing
source("simple.linear.regression.R")
# ===== OMITTED VARIABLE BIAS ==================================================
# What happens when your regressor is correlated with another determinant of y?
# if we remember,there was another determinant of Y that was correlated with X1
# check "simulation.R"
# in our orignal model, we put it aside
# let us take it into account in a new model
mult.mod <- lm(Y ~ X1 + X2 , data = sim.data)
# How do the results from the first model and this one differ
linearMod
# Call:
# lm(formula = Y ~ X1, data = sim.data)
# Coefficients:
#  (Intercept)           X1
#       105.308        4.436
mult.mod
# Call:
#   lm(formula = Y ~ X1 + X2, data = sim.data)
# Coefficients:
#   (Intercept)           X1           X2
#       -20.424        3.451        3.009
# So, what do we observe?
# the coefficient of X1 lowers once X2 is included
# This is what we call an omitted variable bias
# We omitted a variable that both explains Y and is correlated with X1 in linearMod
# Hence we created a model that overestimated the influence of X1 on Y
# That is the omitted variable bias
# It is why in most cases, a multiple linear model suits a dataset better
# ==============================================================================
# ==== MULTIPLE REGRESSION MODEL ===============================================
# obtain information on the mult.mod coefficients using summary
summary(mult.mod)$coef
# Estimate Std. Error   t value     Pr(>|t|)
# (Intercept) -20.424381 14.8175761 -1.378389 1.746096e-01
# X1            3.451497  0.3143780 10.978812 1.443931e-14
# X2            3.009099  0.3042933  9.888811 4.588627e-13
# => Y = -20.42 + 3.45X1 + 3.009X2
# ==============================================================================
# ==== MEASURES OF FIT IN MULTIPLE REGRESSION ==================================
# What is the R^2 and Adjusted R^2 of our multiple regression model
# find out with summary
summary(mult.mod)
# Residual standard error: 4.162 on 47 degrees of freedom
# Multiple R-squared:  0.8716,	Adjusted R-squared:  0.8661
# F-statistic: 159.5 on 2 and 47 DF,  p-value: < 2.2e-16
# Let us compare this effect size with that of the single regression model
summary(linearMod)
# Residual standard error: 7.229 on 48 degrees of freedom
# Multiple R-squared:  0.6044,	Adjusted R-squared:  0.5962
# F-statistic: 73.35 on 1 and 48 DF,  p-value: 3.133e-11
# 0.8661 > 0.5962
# The multiple regression model  has a much higher adjusted R-squared
# ==============================================================================
# ==== OLS ASSUMPTIONS IN MULTIPLE REGRESSION ==================================
# The assumptions  of a multiple regression model are similar to that for a
# linear regression, except that of multicollinearity
# Multiple regressions require that no two regressors are perfectly correlated
# In such cases,R does not compute coefficients for the perfectly correlated regressor
# The higher the correlation between regressors, the higher the variance of ..
# ... model coefficient estimates.
# to check the variance inflation, we use the vif function from the DAAG package
vif(mult.mod)
#     X1     X2
# 1.1114 1.1114
# a vif of more than 4 is cause for concern
# for our model, we can see that the regressors are not multicorrelated
# to visualize this relationship, we can use the corrplot function
# this is obtained from the Rpackpage "corrplot"
# we plot the regressors without the response variable
corrplot(cor(sim.data[,-3]))
# from the plot, we see that X1 and X2 are not very correlated
# ==============================================================================
# ==== DISTRIBUTION OF OLS ESTIMATES ===========================================
# we need to load the R package "MASS"
# initialize vector of coefficients
coefs <- cbind("hat_beta_1" = numeric(10000), "hat_beta_2" = numeric(10000))
# set seed for reproducibility
set.seed(1)
# loop sampling and estimation of our model
# create a loop that randomly generates numbers
# this will be done in the same way we simulated data
for (i in 1:10000) {
A<- rmvnorm(50, c(25, 50), sigma = cbind(c(5, 1.25), c(1.25, 10)))
e <- rnorm(50, sd = 5)
B <- 5 + 2.5 * X[, 1] + 3 * X[, 2] + e
# store coefficients in vector for coefficients
coefs[i,] <- lm(B ~ A[,1] + A[,2])$coefficients[- 1]
}
# compute density estimates
kde <- kde2d(coefs[,1], coefs[,2])
# plot density estimate
persp(kde,
theta = 30,
phi = 30,
xlab = "beta_1",
ylab = "beta_2",
zlab = "Est. Density",
main = "Distribution of Model Estimates")
# the plot shows a distribution of coefficients estimated by R
# ==============================================================================
# ===== CONCLUSION =============================================================
# Our original simulated data showed a true model of :
# Y = 5 + 2.5 * X[, 1] + 3 * X[, 2] + u
# linearMod estimated that Y= 105.308 + 4.436*X[,1]
# mult.mod estimated that Y = -20.424 + 3.451*X[,1] + 3.009*X[,2]
# These are both estimations of the true model
# we can progress to more complex forms of modeling
# And these might allow for better estimations of the true model
# ==============================================================================
#
# loop sampling and estimation of our model
# create a loop that randomly generates numbers
# this will be done in the same way we simulated data
for (i in 1:10000) {
A<- rmvnorm(50, c(25, 50), sigma = cbind(c(5, 1.25), c(1.25, 10)))
e <- rnorm(50, sd = 5)
B <- 5 + 2.5 * A[, 1] + 3 * A[, 2] + e
# store coefficients in vector for coefficients
coefs[i,] <- lm(B ~ A[,1] + A[,2])$coefficients[- 1]
}
# compute density estimates
kde <- kde2d(coefs[,1], coefs[,2])
head(kde)
# plot density estimate
persp(kde,
theta = 30,
phi = 30,
xlab = "beta_1",
ylab = "beta_2",
zlab = "Est. Density",
main = "Distribution of Model Estimates")
slr.plot <- scatter(x= sim.data$X1, y= sim.data$Y, main ="X1~ Y",
pch =16, col = 'red', xlab = "X1", ylab = "Y")
slr.plot <- scatterplot(x= sim.data$X1, y= sim.data$Y, main ="X1~ Y",
pch =16, col = 'red', xlab = "X1", ylab = "Y")
slr.plot <- scatter.smooth(x= sim.data$X1, y= sim.data$Y, main ="X1~ Y",
pch =16, col = 'red', xlab = "X1", ylab = "Y")
slr.plot <- plot(x= sim.data$X1, y= sim.data$Y, main ="X1~ Y",
pch =16, col = 'red', xlab = "X1", ylab = "Y")
slr.plot <- plot(x= sim.data$X1, y= sim.data$Y, main ="X1~ Y",
pch =16, col = 'red', xlab = "X1", ylab = "Y")
abline(linearMod)
pdf("4.figures/simple.linear.regression.pdf", height = 5, width = 8)
slr.plot <- plot(x= sim.data$X1, y= sim.data$Y, main ="X1~ Y",
pch =16, col = 'red', xlab = "X1", ylab = "Y")
abline(linearMod)
par(mfrow = c(1,2))
boxplot(sim.data$X1, main = "X1",
sub = paste("Outlier rows:",
boxplot.stats(sim.data$X1)$out))
boxplot(sim.data$Y, main = "Y",
sub = paste("Outlier rows:",
boxplot.stats(sim.data$Y)$out))
plot(density(sim.data$X1), main = "Density Plot:X1",
ylab = "Frequency", sub = paste("Skewness:",
round(e1071::skewness(sim.data$X1),2)))
polygon(density(sim.data$X1), col = 'red')
plot(density(sim.data$Y), main = "Density Plot:Y",
ylab = "Frequency", sub = paste("Skewness:",
round(e1071::skewness(sim.data$Y),2)))
polygon(density(sim.data$Y), col = 'red')
par(mfrow = c(1,1))
cvResults <- suppressWarnings( CVlm (sim.data, form.lm = Y ~ X1,
m=10, dots=FALSE, seed=29,
legend.pos="topleft",  printit=FALSE,
main="10-Fold Cross Validation"))
dev.off()
#
#
# DATA MANIPULATION
# Retrieving the data I need
#
# ======== RAW DATA ============================================================
# let's read the raw data from data.raw
debt.raw <- read.csv(paste(path.data.raw,"debt.raw.csv",
sep = ""),
stringsAsFactors = FALSE,
strip.white = TRUE)
# inspect series names
debt.raw$ï..series.name <- debt.raw$series.name
str(debt.raw)
#
#
# DATA MANIPULATION
# Retrieving the data I need
#
# ======== RAW DATA ============================================================
# let's read the raw data from data.raw
debt.raw <- read.csv(paste(path.data.raw,"debt.raw.csv",
sep = ""),
stringsAsFactors = FALSE,
strip.white = TRUE)
unique(debt.raw$ï..series.name)
chosen.var <- subset(debt.raw,ï..series.name == x )
# making subsets of data with variables to be used
# use a function
# x represents the unique Series Name
getvar <- function(x){
chosen.var <- subset(debt.raw,ï..series.name == x )
return(chosen.var)
}
# =============================================================================
#
# MAIN
#
R.version.string
#   "R version 4.0.3 (2020-10-10)"
#
# =============================================================================
# NOTES
# open the scripts in the orders in which they are numbered.
# - "2.simulation.R" shows how the data used for the analyses was simulated
# - "3.simple.linear.regression.R" shows the processes involved in conducting
# a simple linear regression
# - "4.assumptions.R" explores the assumptions of a linear regression
# - "5.multiple.regression.R" shows the processes for a multiple linear
# regresssion
# - "6.data.manipulation.R" shows how the debt data obtained from the World Bank
# was cleaned
# - "7.data.analysis.R" shows the attempts at analysing the clean debt data
# - "8.saveplots.R" is a script where plots were saved into pdf's
# =============================================================================
# Set the working dir to retrace where files should be
# in case the code fails
wk.dir <- getwd() #
# =============================================================================
# ---- libraries ----
# install libraries needed for the project
# load the libraries needed for the project to run
library(mvtnorm)
library(e1071)
library(lattice)
library(DAAG)
library(ggplot2)
library(DataCombine)
library(gvlma)
library(AER)
library(MASS)
library(corrplot)
# =============================================================================
# --- folder management ---
# names of project folders ("figures", "data.raw","data.clean","results")
# store names of the folders in an object
folder.names <- c("1.data.raw","2.data.clean", "3.results","4.figures")
# make the folders if they don't exist yet.
for(i in 1:length(folder.names)){
if(file.exists(folder.names[i]) == FALSE){
dir.create(folder.names[i])
}
}
#Store in an object the file path to these folders so we can
# read from them and write to them.
path.data.raw <- paste(wk.dir, "/", folder.names[1], "/", sep = "")
path.data.clean <- paste(wk.dir, "/", folder.names[2], "/", sep = "")
path.results <- paste(wk.dir, "/", folder.names[3], "/", sep = "")
path.fig <- paste(wk.dir, "/",folder.names[4], "/", sep = "")
# =============================================================================
# --- run scripts ---
# ==== end =================================================================
#
#
# DATA MANIPULATION
# Retrieving the data I need
#
# ======== RAW DATA ============================================================
# let's read the raw data from data.raw
debt.raw <- read.csv(paste(path.data.raw,"debt.raw.csv",
sep = ""),
stringsAsFactors = FALSE,
strip.white = TRUE)
# inspect the structure of data.raw
str(debt.raw)
head(debt.raw)
# inspect series names
str(debt.raw)
unique(debt.raw$ï..series.name)
# make columns with yearly data numeric
for (i in 5:ncol(debt.raw)){
debt.raw[,i] <- as.numeric(as.character(debt.raw[,i]))
}
# ==============================================================================
# ======== DATA  MANIPULATION ==================================================
# making subsets of data with variables to be used
# use a function
# x represents the unique Series Name
getvar <- function(x){
chosen.var <- subset(debt.raw,ï..series.name == x )
return(chosen.var)
}
# creating data frames of the variables to be used
debt.stock <- getvar("External debt stocks (% of GNI)")
gni <- getvar("GNI (current US$)" )
debt.service.cap <- getvar("Total debt service (% of exports of goods, services and primary income)")
fdi <- getvar("Foreign direct investment, net (BoP, current US$)" )
portfolio <- getvar("Portfolio investment, net (BoP, current US$)")
export <- getvar("Exports of goods and services (current US$)")
import <- getvar("Imports of goods and services (current US$)")
str(export)
# maybe I should create a function that find fi. values for each year
fi <- function(i){
e.val<- export[,i]* 0.25
i.val <- import[,i] * 0.25
fdi.val <- fdi[,i] * 0.3
p.val <- portfolio[,i] * 0.2
fi.val <- e.val + i.val + fdi.val + p.val
return(fi.val)
}
# financial integration values for the years to be compared
fi.2009 <- fi(6)
fi.2014 <- fi(11)
fi.2019 <- fi(16)
# make dataframe with country names and others only
debt.clean <- data.frame(rep(export$Country.Name,3))
# make a vector of years
years <- c(rep(2009, 48),rep(2014,48), rep(2019,48))
# add this vector as a column in debt. clean
debt.clean$years <- years
# add financial integration data
debt.clean$financial.integration <- c(fi.2009,fi.2014, fi.2019)
str(debt.stock)
# retrieve debt stock figures for 2009, 2014, and 2019
ds.2009 <- debt.stock[,6]
ds.2014 <- debt.stock[,11]
ds.2019 <- debt.stock[,16]
# add debt stock data to debt.clean
debt.clean$debt.stock <- c(ds.2009, ds.2014, ds.2019)
# retrieve debt service capabilities figures
dsc.2009 <- debt.service.cap[,6]
dsc.2014 <- debt.service.cap [,11]
dsc.2019 <- debt.service.cap [,16]
# add debt service data to debt.clean
debt.clean$debt.service.cap <- c(dsc.2009,dsc.2014,dsc.2019)
# set column names
colnames(debt.clean) <- c("country.name", "years", "financial.integration",
"debt.stock", "debt.service.cap")
# write debt.clean as a csv in path.data.clean
write.csv(debt.clean, paste(path.data.clean,"debt.clean.csv", sep = ""),
row.names = FALSE)
# make data set that drops data with NA financial integration
debt.new <- debt.clean[complete.cases(debt.clean),]
# write a new csv with complete data
write.csv(debt.new, paste(path.data.clean,"debt.complete.csv", sep = ""),
row.names = FALSE)
# ==============================================================================
